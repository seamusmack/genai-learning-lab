{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "IY1MDblAlyIG",
      "metadata": {
        "id": "IY1MDblAlyIG"
      },
      "source": [
        "# **LangChain Loader, Splitter, and Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kGVt_ry_Mwyj",
      "metadata": {
        "id": "kGVt_ry_Mwyj"
      },
      "source": [
        "# __Description:__\n",
        "In this activity, you will implement the functionalities of LangChain’s loaders, splitters, and embeddings.\n",
        "The two files in the tutorial serve as practical examples of real-world data that one might encounter in natural language processing tasks. They are:\n",
        "\n",
        "•\tThe **state_of_union.txt** file, which contains transcripts of the United States’ State of the Union Addresses, represents a large text document that can be loaded and processed.\n",
        "\n",
        "•\tThe **michael_resume.pdf** file, an open source resume, represents a common type of document that one might analyze for tasks such as resume screening or information extraction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TIxRkrn-l5qc",
      "metadata": {
        "id": "TIxRkrn-l5qc"
      },
      "source": [
        "# **Steps to Perform:**\n",
        "\n",
        "\n",
        "1.   Import the Necessary Modules\n",
        "2.   Load Text Data from a File Using TextLoader\n",
        "3.   Load PDFs from the Internet Using PyPDFLoader\n",
        "4.   Split the Documents Using RecursiveCharacterTextSplitter\n",
        "5.   Embed the Documents Using HuggingFaceEmbeddings and Print the Length of the Embedding\n",
        "6.   Embed the Documents Using OpenAIEmbeddings and Print the Length of the Embedding\n",
        "7.   Create a FAISS Instance\n",
        "8.   Perform a Similarity Search on the FAISS Instance\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd37b94",
      "metadata": {},
      "source": [
        "# Demo: LangChain Document Loading → Splitting → Embedding → FAISS Search\n",
        "\n",
        "## Overview\n",
        "This demo walks through a full Retrieval-Augmented Generation (RAG) data preparation pipeline using **LangChain** with the modern namespace imports, **OpenAI embeddings**, and a **FAISS** vector store.  \n",
        "It demonstrates how to load text/PDF files, split them into manageable chunks, embed them into vectors, store them in a searchable index, and perform both standard and diversified (MMR) similarity searches.\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-Step Process\n",
        "\n",
        "### **Step 1 — Install Required Packages**\n",
        "Install the core LangChain libraries, OpenAI integration, FAISS vector store, and supporting tools.\n",
        "```bash\n",
        "pip install -U langchain langchain-community langchain-text-splitters \\\n",
        "    langchain-openai faiss-cpu pypdf tiktoken chromadb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6xcPQbtUNQU6",
      "metadata": {
        "id": "6xcPQbtUNQU6"
      },
      "source": [
        "# **Step 1: Import the Necessary Modules**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b6940571-b490-4a34-9586-f8f2de73d967",
      "metadata": {
        "id": "b6940571-b490-4a34-9586-f8f2de73d967",
        "outputId": "62cd167c-982f-4adf-f512-a4d1a3eb67db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-text-splitters in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.9)\n",
            "Requirement already satisfied: langchain-openai in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.29)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting pypdf\n",
            "  Using cached pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.11.0)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.16-cp39-abi3-win_amd64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (2.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-openai) (1.98.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting importlib-resources (from chromadb)\n",
            "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
            "Collecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (3.11.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (14.0.0)\n",
            "Collecting jsonschema>=4.19.0 (from chromadb)\n",
            "  Downloading jsonschema-4.25.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\jim\\appdata\\roaming\\python\\python311\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\jim\\appdata\\roaming\\python\\python311\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading rpds_py-0.27.0-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
            "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.33.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jim\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-win_amd64.whl (14.9 MB)\n",
            "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
            "   -------------- ------------------------- 5.2/14.9 MB 29.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  14.7/14.9 MB 44.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 14.9/14.9 MB 36.0 MB/s  0:00:00\n",
            "Using cached pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "Downloading chromadb-1.0.16-cp39-abi3-win_amd64.whl (19.6 MB)\n",
            "   ---------------------------------------- 0.0/19.6 MB ? eta -:--:--\n",
            "   ------------------------- -------------- 12.6/19.6 MB 60.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 19.6/19.6 MB 49.7 MB/s  0:00:00\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
            "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading jsonschema-4.25.0-py3-none-any.whl (89 kB)\n",
            "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.9/1.9 MB 13.4 MB/s  0:00:00\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl (41 kB)\n",
            "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl (12.7 MB)\n",
            "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
            "   ------------------------------- -------- 10.0/12.7 MB 47.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.7/12.7 MB 33.1 MB/s  0:00:00\n",
            "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl (35 kB)\n",
            "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.27.0-cp311-cp311-win_amd64.whl (230 kB)\n",
            "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
            "Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl (292 kB)\n",
            "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml): started\n",
            "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=fa7da64fbca18d2cd1f7866b890af42445b5d33e508022d59557a18e7be40a74\n",
            "  Stored in directory: c:\\users\\jim\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, zipp, websockets, websocket-client, shellingham, rpds-py, pyreadline3, pyproject_hooks, pypdf, pybase64, pyasn1, overrides, opentelemetry-proto, oauthlib, mmh3, importlib-resources, httptools, googleapis-common-protos, faiss-cpu, click, cachetools, bcrypt, backoff, watchfiles, uvicorn, rsa, requests-oauthlib, referencing, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, importlib-metadata, humanfriendly, build, typer, opentelemetry-api, jsonschema-specifications, google-auth, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, jsonschema, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "\n",
            "   - --------------------------------------  2/47 [zipp]\n",
            "   -- -------------------------------------  3/47 [websockets]\n",
            "   -- -------------------------------------  3/47 [websockets]\n",
            "   -- -------------------------------------  3/47 [websockets]\n",
            "   -- -------------------------------------  3/47 [websockets]\n",
            "   --- ------------------------------------  4/47 [websocket-client]\n",
            "   --- ------------------------------------  4/47 [websocket-client]\n",
            "   --- ------------------------------------  4/47 [websocket-client]\n",
            "   ----- ----------------------------------  6/47 [rpds-py]\n",
            "   ----- ----------------------------------  7/47 [pyreadline3]\n",
            "   ----- ----------------------------------  7/47 [pyreadline3]\n",
            "   ------ ---------------------------------  8/47 [pyproject_hooks]\n",
            "   ------- --------------------------------  9/47 [pypdf]\n",
            "   ------- --------------------------------  9/47 [pypdf]\n",
            "   ------- --------------------------------  9/47 [pypdf]\n",
            "   -------- ------------------------------- 10/47 [pybase64]\n",
            "   --------- ------------------------------ 11/47 [pyasn1]\n",
            "   --------- ------------------------------ 11/47 [pyasn1]\n",
            "   ---------- ----------------------------- 12/47 [overrides]\n",
            "   ----------- ---------------------------- 13/47 [opentelemetry-proto]\n",
            "   ----------- ---------------------------- 13/47 [opentelemetry-proto]\n",
            "   ----------- ---------------------------- 14/47 [oauthlib]\n",
            "   ----------- ---------------------------- 14/47 [oauthlib]\n",
            "   ----------- ---------------------------- 14/47 [oauthlib]\n",
            "   ----------- ---------------------------- 14/47 [oauthlib]\n",
            "   ----------- ---------------------------- 14/47 [oauthlib]\n",
            "   ------------ --------------------------- 15/47 [mmh3]\n",
            "   ------------- -------------------------- 16/47 [importlib-resources]\n",
            "   ------------- -------------------------- 16/47 [importlib-resources]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   --------------- ------------------------ 18/47 [googleapis-common-protos]\n",
            "   ---------------- ----------------------- 19/47 [faiss-cpu]\n",
            "   ---------------- ----------------------- 19/47 [faiss-cpu]\n",
            "   ---------------- ----------------------- 19/47 [faiss-cpu]\n",
            "   ---------------- ----------------------- 19/47 [faiss-cpu]\n",
            "   ----------------- ---------------------- 20/47 [click]\n",
            "   ----------------- ---------------------- 20/47 [click]\n",
            "   ------------------- -------------------- 23/47 [backoff]\n",
            "   -------------------- ------------------- 24/47 [watchfiles]\n",
            "   --------------------- ------------------ 25/47 [uvicorn]\n",
            "   --------------------- ------------------ 25/47 [uvicorn]\n",
            "   --------------------- ------------------ 25/47 [uvicorn]\n",
            "   ---------------------- ----------------- 26/47 [rsa]\n",
            "   ---------------------- ----------------- 26/47 [rsa]\n",
            "   ---------------------- ----------------- 26/47 [rsa]\n",
            "   ---------------------- ----------------- 26/47 [rsa]\n",
            "   ---------------------- ----------------- 26/47 [rsa]\n",
            "   ---------------------- ----------------- 27/47 [requests-oauthlib]\n",
            "   ----------------------- ---------------- 28/47 [referencing]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------ --------------- 29/47 [pyasn1-modules]\n",
            "   ------------------------- -------------- 30/47 [posthog]\n",
            "   ------------------------- -------------- 30/47 [posthog]\n",
            "   ------------------------- -------------- 30/47 [posthog]\n",
            "   ----------------- --------- 31/47 [opentelemetry-exporter-otlp-proto-common]\n",
            "   --------------------------- ------------ 32/47 [importlib-metadata]\n",
            "   ---------------------------- ----------- 33/47 [humanfriendly]\n",
            "   ---------------------------- ----------- 33/47 [humanfriendly]\n",
            "   ---------------------------- ----------- 34/47 [build]\n",
            "   ----------------------------- ---------- 35/47 [typer]\n",
            "   ----------------------------- ---------- 35/47 [typer]\n",
            "   ------------------------------ --------- 36/47 [opentelemetry-api]\n",
            "   ------------------------------ --------- 36/47 [opentelemetry-api]\n",
            "   ------------------------------ --------- 36/47 [opentelemetry-api]\n",
            "   -------------------------------- ------- 38/47 [google-auth]\n",
            "   -------------------------------- ------- 38/47 [google-auth]\n",
            "   -------------------------------- ------- 38/47 [google-auth]\n",
            "   -------------------------------- ------- 38/47 [google-auth]\n",
            "   -------------------------------- ------- 38/47 [google-auth]\n",
            "   -------------------------------- ------- 38/47 [google-auth]\n",
            "   --------------------------------- ------ 39/47 [coloredlogs]\n",
            "   --------------------------------- ------ 39/47 [coloredlogs]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------- ---- 40/47 [opentelemetry-semantic-conventions]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ---------------------------------- ----- 41/47 [onnxruntime]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ----------------------------------- ---- 42/47 [kubernetes]\n",
            "   ------------------------------------ --- 43/47 [jsonschema]\n",
            "   ------------------------------------ --- 43/47 [jsonschema]\n",
            "   ------------------------------------ --- 43/47 [jsonschema]\n",
            "   ------------------------------------ --- 43/47 [jsonschema]\n",
            "   ------------------------------------- -- 44/47 [opentelemetry-sdk]\n",
            "   ------------------------------------- -- 44/47 [opentelemetry-sdk]\n",
            "   ------------------------------------- -- 44/47 [opentelemetry-sdk]\n",
            "   ------------------------------------- -- 44/47 [opentelemetry-sdk]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------  46/47 [chromadb]\n",
            "   ---------------------------------------- 47/47 [chromadb]\n",
            "\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chromadb-1.0.16 click-8.2.1 coloredlogs-15.0.1 durationpy-0.10 faiss-cpu-1.11.0.post1 google-auth-2.40.3 googleapis-common-protos-1.70.0 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 jsonschema-4.25.0 jsonschema-specifications-2025.4.1 kubernetes-33.1.0 mmh3-5.2.0 oauthlib-3.3.1 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 posthog-5.4.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pypdf-5.9.0 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 referencing-0.36.2 requests-oauthlib-2.0.0 rpds-py-0.27.0 rsa-4.9.1 shellingham-1.5.4 typer-0.16.0 uvicorn-0.35.0 watchfiles-1.1.0 websocket-client-1.8.0 websockets-15.0.1 zipp-3.23.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -U langchain langchain-community langchain-text-splitters \\\n",
        "    langchain-openai faiss-cpu pypdf tiktoken chromadb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d46460",
      "metadata": {},
      "source": [
        "Step 2 — Configuration\n",
        "Choose USE_OPENAI = True to use OpenAI embeddings (default for class).\n",
        "\n",
        "Set your OPENAI_API_KEY (project-scoped key for demo).\n",
        "\n",
        "Define file paths for the .txt and .pdf files and the directory for persistent storage (optional Chroma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ecf275",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2 \n",
        "\n",
        "# --- Configuration ---\n",
        "USE_OPENAI = True  # OpenAI embeddings by default for class demo\n",
        "\n",
        "# Paste your project-scoped OpenAI key here\n",
        "OPENAI_API_KEY = \"YOUR_OPENAI_KEY\"\n",
        "\n",
        "# Demo file locations\n",
        "DATA_DIR   = \"./data\"\n",
        "TEXT_PATH  = f\"{DATA_DIR}/state_of_union.txt\"\n",
        "PDF_PATH   = f\"{DATA_DIR}/michael_resume.pdf\"\n",
        "CHROMA_DIR = \"./chroma_demo_store\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a343fef1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text file created at: ./data/state_of_union.txt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not Path(TEXT_PATH).exists():\n",
        "    sample_text = \"\"\"\n",
        "    The United States is a nation of possibilities. We will build, innovate, and lead.\n",
        "    This placeholder exists so the demo runs even without a real file.\n",
        "    \"\"\".strip()\n",
        "    with open(TEXT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(sample_text)\n",
        "\n",
        "print(\"Text file created at:\", TEXT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc429e01",
      "metadata": {},
      "source": [
        "Step 3 — Load Text Documents\n",
        "Use TextLoader from langchain_community.document_loaders to load a .txt file.\n",
        "\n",
        "Preview the first part of the document to confirm loading worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ca979f-04fd-4c58-8798-034292d94377",
      "metadata": {
        "id": "a2ca979f-04fd-4c58-8798-034292d94377",
        "outputId": "b13cdbd8-40a8-428b-dd34-3d88d59abf93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 document(s)\n",
            "The United States is a nation of possibilities. We will build, innovate, and lead.\n",
            "    This placeholder exists so the demo runs even without a real file.\n"
          ]
        }
      ],
      "source": [
        "# Step 3 \n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Load the text file into a Document list\n",
        "text_loader = TextLoader(TEXT_PATH, encoding=\"utf-8\")\n",
        "text_docs = text_loader.load()\n",
        "\n",
        "# Show the first 200 characters of the first document\n",
        "print(f\"Loaded {len(text_docs)} document(s)\")\n",
        "print(text_docs[0].page_content[:200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "63ba6b83-4196-4673-829e-d6874bd742a8",
      "metadata": {
        "id": "63ba6b83-4196-4673-829e-d6874bd742a8",
        "outputId": "02ee8210-e2fc-4dd2-af90-a43c7f54acb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No PDF found at ./data/michael_resume.pdf\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from pathlib import Path\n",
        "\n",
        "pdf_docs = []\n",
        "if Path(PDF_PATH).exists():\n",
        "    pdf_loader = PyPDFLoader(PDF_PATH)\n",
        "    pdf_docs = pdf_loader.load_and_split()\n",
        "    print(f\"Loaded {len(pdf_docs)} page(s) from PDF\")\n",
        "    print(pdf_docs[0].page_content[:200])\n",
        "else:\n",
        "    print(\"No PDF found at\", PDF_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2IzUICifOBqd",
      "metadata": {
        "id": "2IzUICifOBqd"
      },
      "source": [
        "# **Step 4: Split the Documents Using RecursiveCharacterTextSplitter**\n",
        "\n",
        "\n",
        "*   Split the PDF pages into smaller chunks and print the number of chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88edad5e",
      "metadata": {},
      "source": [
        "Step 4 — Load PDF Documents (Optional)\n",
        "Use PyPDFLoader from langchain_community.document_loaders to load and split a PDF by page.\n",
        "\n",
        "Preview a sample page.\n",
        "\n",
        "If no PDF is present, skip this step — the demo still works with text-only data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12653170-35dd-4a70-a997-380006660a23",
      "metadata": {
        "id": "12653170-35dd-4a70-a997-380006660a23",
        "outputId": "74f5cad8-0c2b-415c-fe5e-2d024dcd347a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source docs: 1\n",
            "Chunks: 1\n",
            "Sample chunk:\n",
            " The United States is a nation of possibilities. We will build, innovate, and lead.\n",
            "    This placeholder exists so the demo runs even without a real file.\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Combine all available documents (text + pdf)\n",
        "all_docs = text_docs + pdf_docs\n",
        "\n",
        "# Initialize splitter (adjust chunk size/overlap as needed)\n",
        "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "\n",
        "# Perform the split\n",
        "split_texts = doc_splitter.split_documents(all_docs)\n",
        "\n",
        "print(f\"Source docs: {len(all_docs)}\")\n",
        "print(f\"Chunks: {len(split_texts)}\")\n",
        "print(\"Sample chunk:\\n\", split_texts[0].page_content[:300])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rawkX5UeOe8e",
      "metadata": {
        "id": "rawkX5UeOe8e"
      },
      "source": [
        "# **Step 5: Embed the Documents Using HuggingFaceEmbeddings and Print the Length of the Embedding**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d79540bc",
      "metadata": {},
      "source": [
        "Step 5 — Combine and Split Documents\n",
        "Combine all text and PDF Document objects into one list.\n",
        "\n",
        "Use RecursiveCharacterTextSplitter from langchain_text_splitters to break content into chunks.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "chunk_size=800–1024 characters\n",
        "\n",
        "chunk_overlap=64–120 characters\n",
        "\n",
        "Overlap helps preserve context across chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "54e6ffdc-3c5e-468f-9ed7-f703cb9ddad8",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d64a277a50c74195bc764148604e9a99",
            "4c7c32e3807b43178685892a533c5571",
            "9ec53712abde4c6ab8f6f33aecf4d7bc",
            "50597f9d2d594efda364be18cf17a180",
            "fdcab798b77f45739885570f45132267",
            "151f77060c5a407a9dc523aaf4888221",
            "ddcbbb0397604f5686c8d783a63a29f2",
            "54abd02225434d17bc00568232ada7e0",
            "7f85963918f34167b09fdaf07e907249",
            "d604d2e0d6734b2faaef3e65c8ab06b7",
            "93c4c9fe19e94f1493bc31e66ddaede5",
            "b33005a189c64bb4a430753253ebb9c8",
            "7274d1c1caa64b1f9be31585ecd67430",
            "49d10e1d64004d469bc94847dd1d1037",
            "46f7a294befb41b595619596f168657a"
          ]
        },
        "id": "54e6ffdc-3c5e-468f-9ed7-f703cb9ddad8",
        "outputId": "6883936f-1343-4b7b-f336-4bc0ef7bd236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding vector length: 1536\n",
            "First 10 values: [0.008483029901981354, 0.027606286108493805, 0.022286172956228256, 0.05445463955402374, -0.014859877526760101, -0.058594122529029846, -0.008978602476418018, 0.0401996485888958, 0.018642259761691093, 0.016324730589985847]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "# Ensure API key is set\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Test embedding size with first chunk\n",
        "sample_text = split_texts[0].page_content\n",
        "embed_result = embedder.embed_query(sample_text)\n",
        "\n",
        "print(\"Embedding vector length:\", len(embed_result))\n",
        "print(\"First 10 values:\", embed_result[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3x-PVKuQO6sh",
      "metadata": {
        "id": "3x-PVKuQO6sh"
      },
      "source": [
        "# **Step 6: Embed the Documents Using OpenAIEmbeddings and Print the Length of the Embedding**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03a6484",
      "metadata": {},
      "source": [
        "Step 6 — Embed the Chunks\n",
        "Use OpenAIEmbeddings(model=\"text-embedding-3-small\") for compact, cost-effective embeddings.\n",
        "\n",
        "Embed the split text chunks into numerical vectors.\n",
        "\n",
        "Test with a single chunk to confirm vector length (e.g., 1536 dimensions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ff51eb44-7891-4625-b69a-2071ed81fdef",
      "metadata": {
        "id": "ff51eb44-7891-4625-b69a-2071ed81fdef",
        "outputId": "fa0fa7a4-a1e4-45b0-d4d8-cb911e191dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding vector length: 1536\n",
            "First 10 values: [0.008483029901981354, 0.027606286108493805, 0.022286172956228256, 0.05445463955402374, -0.014859877526760101, -0.058594122529029846, -0.008978602476418018, 0.0401996485888958, 0.018642259761691093, 0.016324730589985847]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "# Ensure API key is set\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Initialize OpenAI embeddings with explicit model\n",
        "openai_embed = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Take the first chunk’s text for testing\n",
        "sample_text = split_texts[0].page_content\n",
        "\n",
        "# Embed (returns a list of vectors, one per input string)\n",
        "openai_embed_result = openai_embed.embed_documents([sample_text])\n",
        "\n",
        "print(\"Embedding vector length:\", len(openai_embed_result[0]))\n",
        "print(\"First 10 values:\", openai_embed_result[0][:10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MEtwmKkJPII8",
      "metadata": {
        "id": "MEtwmKkJPII8"
      },
      "source": [
        "# **Step 7: Create a FAISS Instance**\n",
        "\n",
        "*   Create a FAISS instance using the split texts and the OpenAIEmbeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d34d462",
      "metadata": {},
      "source": [
        "Step 7 — Create a FAISS Vector Store\n",
        "Use FAISS.from_documents(chunks, embedder) to store the vectors in memory.\n",
        "\n",
        "FAISS is fast and ideal for demos or temporary retrieval indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f90478cb-1356-4a0d-8250-e055c9dcf511",
      "metadata": {
        "id": "f90478cb-1356-4a0d-8250-e055c9dcf511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS store created\n",
            "Number of vectors stored: 1\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Create FAISS instance from split chunks and OpenAI embeddings\n",
        "faiss_store = FAISS.from_documents(split_texts, openai_embed)\n",
        "\n",
        "print(\"FAISS store created\")\n",
        "print(\"Number of vectors stored:\", len(split_texts))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XTWZ3kgJPT5J",
      "metadata": {
        "id": "XTWZ3kgJPT5J"
      },
      "source": [
        "# **Step 8: Perform a Similarity Search on the FAISS Instance**\n",
        "\n",
        "\n",
        "*   Print the top two most similar documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f86c75f",
      "metadata": {},
      "source": [
        "Step 8 — Perform a Similarity Search\n",
        "Use similarity_search(query, k) to retrieve the most relevant chunks for a query.\n",
        "\n",
        "Optionally use similarity_search_with_score to view relevance scores.\n",
        "\n",
        "Print results in a readable format for class discussion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5f78e000-488a-47e2-9b0d-9bfa215703b8",
      "metadata": {
        "id": "5f78e000-488a-47e2-9b0d-9bfa215703b8",
        "outputId": "6da6683d-f734-4d61-9a23-7c50a507a9b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "The United States is a nation of possibilities. We will build, innovate, and lead.\n",
            "    This placeholder exists so the demo runs even without a real file. ...\n"
          ]
        }
      ],
      "source": [
        "# Define your query\n",
        "query = \"What is the candidate's skill set?\"\n",
        "\n",
        "# Perform search (top 2 matches)\n",
        "search_results = faiss_store.similarity_search(query, k=2)\n",
        "\n",
        "# Nicely format the output\n",
        "for i, doc in enumerate(search_results, start=1):\n",
        "    print(f\"\\n--- Result {i} ---\")\n",
        "    print(doc.page_content[:300], \"...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afee11fc",
      "metadata": {},
      "source": [
        "Step 9 — Perform Max Marginal Relevance (MMR) Search\n",
        "Use max_marginal_relevance_search(query, k, fetch_k, lambda_mult) to retrieve results that balance relevance and diversity.\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "fetch_k: number of candidates to consider before selecting the final set.\n",
        "\n",
        "lambda_mult:\n",
        "\n",
        "0.0 → max diversity\n",
        "\n",
        "1.0 → max relevance\n",
        "\n",
        "0.3–0.7 → balanced\n",
        "\n",
        "Useful for avoiding duplicate or near-duplicate chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "af9f075a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MMR returned 1 results\n",
            "\n",
            "--- MMR Result 1 ---\n",
            "The United States is a nation of possibilities. We will build, innovate, and lead.\n",
            "    This placeholder exists so the demo runs even without a real file. ...\n"
          ]
        }
      ],
      "source": [
        "# MMR: balances relevance and diversity in retrieved results\n",
        "mmr_results = faiss_store.max_marginal_relevance_search(\n",
        "    query,\n",
        "    k=4,          # final number of results\n",
        "    fetch_k=12,   # pool of candidates to diversify from\n",
        "    lambda_mult=0.5  # 0.0 = max diversity, 1.0 = max relevance\n",
        ")\n",
        "\n",
        "print(f\"MMR returned {len(mmr_results)} results\")\n",
        "for i, doc in enumerate(mmr_results, start=1):\n",
        "    print(f\"\\n--- MMR Result {i} ---\")\n",
        "    print(doc.page_content[:300], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RqaeWpZkQCRc",
      "metadata": {
        "id": "RqaeWpZkQCRc"
      },
      "source": [
        "# **Conclusion**\n",
        "\n",
        "This activity provided a step-by-step guide on how to use LangChain’s loaders, splitters, and embeddings. You now know how to load documents, split them into manageable chunks, embed them into a numerical space, and store these embeddings for efficient similarity searches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62255508",
      "metadata": {},
      "source": [
        "Pipeline modularity: Loader → Splitter → Embedder → Store → Retriever.\n",
        "\n",
        "Chunking strategy: Size and overlap impact retrieval quality.\n",
        "\n",
        "Embeddings choice: OpenAI for convenience, HuggingFace for offline.\n",
        "\n",
        "Vector store choice:\n",
        "\n",
        "FAISS: fast, in-memory (ephemeral).\n",
        "\n",
        "Chroma: persistent, simple local DB.\n",
        "\n",
        "Retrieval strategies:\n",
        "\n",
        "Basic similarity search for pure relevance.\n",
        "\n",
        "MMR search to diversify results and reduce redundancy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
