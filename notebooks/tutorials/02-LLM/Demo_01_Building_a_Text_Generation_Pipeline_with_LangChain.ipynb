{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSNe5dGyCZlq"
   },
   "source": [
    "# **Demo: Building a Text Generation Pipeline with LangChain and Hugging Face's Flan T5 Large Model**\n",
    "\n",
    "In this demo, you will learn how to create a Langchain HuggingFacePipeline for efficient text generation and dive into the creation of a Langchain chain to craft context-aware responses using a custom template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GIv4H_3Ce2F"
   },
   "source": [
    "## **Steps to Perform:**\n",
    "1. Install the Required Libraries and Dependencies\n",
    "2. Authenticate the Hugging Face Account and Set the API Key\n",
    "3. Use the Hugging Face Hub to Load the Flan T5 Large model\n",
    "4. Create a Langchain HuggingFacePipeline for Text Generation\n",
    "5. Build a Chain Using Langchain\n",
    "6. Test and Run the Chain on Few a Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMiJv-SkCo9e"
   },
   "source": [
    "### **Step 1: Install the Required Libraries and Dependencies**\n",
    "\n",
    "\n",
    "*   Install the necessary libraries, including **Langchain**, **Transformers**, and **Torch**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Npz_xvrIxm3",
    "outputId": "e68cbd7d-2122-4e81-ce39-a94ec1a5ed83"
   },
   "outputs": [],
   "source": [
    "## Cell 1\n",
    "!pip install langchain transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip2zy0mDC93N"
   },
   "source": [
    "### **Step 2: Authenticate the Hugging Face Account and Set the API Key**\n",
    "\n",
    "*   Click this link: https://huggingface.co/settings/tokens\n",
    "*   Login or create an account.\n",
    "*   Click on **New token**.\n",
    "*   On the dialog box, give a name and select the role as **write**.\n",
    "*   Copy the access token or API key.\n",
    "*   Replace **Your_HF_Key** with the copied key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_my4ywipYl1"
   },
   "outputs": [],
   "source": [
    "## Cell 2\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = {\"\":}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veodvSN0Dc7r"
   },
   "source": [
    "### **Step 3: Use the Hugging Face Hub to Load the Flan T5 XXL model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9w94guoUkZ4i",
    "outputId": "6d8733fc-1e5f-4525-ee52-590ff0a61202"
   },
   "outputs": [],
   "source": [
    "## Cell 3\n",
    "!pip install -U langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 4\n",
    "import langchain\n",
    "import langchain_community\n",
    "\n",
    "print(\"LangChain version:\", langchain.__version__)\n",
    "print(\"LangChain-Community version:\", langchain_community.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cell 5\n",
    "!pip uninstall -y langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6\n",
    "!pip install --upgrade --force-reinstall langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7\n",
    "import langchain_community\n",
    "print(dir(langchain_community))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8\n",
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4tjani-Zsd7",
    "outputId": "f7bcd9fe-0e7f-4006-8213-adf519708359"
   },
   "outputs": [],
   "source": [
    "## Cell 9\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline, HuggingFaceHub\n",
    "\n",
    "# Example: Uncomment to instantiate the model\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"google/flan-t5-large\",\n",
    "#     model_kwargs={\"temperature\": 1, \"max_length\": 512}\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE90-mP9FHCJ"
   },
   "source": [
    "### **Step 4: Create a Langchain HuggingFacePipeline for Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sfC8putZ4NX"
   },
   "outputs": [],
   "source": [
    "## Cell 10\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 11\n",
    "!pip install wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6PN8DGuFSIn"
   },
   "source": [
    "### **Step 5: Build a Chain Using Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 12\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "# Force PyTorch as the frameworkâ€”critical on Windows\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    framework=\"pt\"  # This avoids ALL tf/keras issues\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "print(\"LLM pipeline ready:\", llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: Wikipedia RAG Retriever\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "# Limit retriever to 1 result to keep context length safe\n",
    "retriever = WikipediaRetriever(top_k_results=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 14\n",
    "def truncate_text(text, max_tokens=450):\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:max_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 15\n",
    "query = \"What is the architectural history of Strasbourg Cathedral?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "context = \" \".join([d.page_content for d in docs])\n",
    "context = truncate_text(context, max_tokens=450)\n",
    "answer = llm(context)\n",
    "print(\"ANSWER:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMzZa8lmqF_i"
   },
   "outputs": [],
   "source": [
    "## Cell 16\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCy0qU74Ff4p"
   },
   "source": [
    "### **Step 6: Test and Run the Chain on Few a Questions**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "VzJK0rXNqKwA",
    "outputId": "af07f3fa-c762-4a9c-f86a-7f857cffe99d"
   },
   "outputs": [],
   "source": [
    "#Question 1\n",
    "question = \"Explain the concept of black holes in simple terms.\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3-3arLF-1Ms"
   },
   "outputs": [],
   "source": [
    "#Question 2\n",
    "question = \"What are the main causes of climate change, and how can we address them?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "UdK6ZB2GBMM0",
    "outputId": "c677ba42-2114-4d6e-af2a-2670b2c38602"
   },
   "outputs": [],
   "source": [
    "#Question 3\n",
    "question = \"Provide a brief overview of the history of artificial intelligence.\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGtdoqadFq-R"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "This sets the stage for your Langchain journey, allowing you to interact with language models seamlessly. In the upcoming demos, we will explore more advanced applications development with Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
