{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "E5vJOngiYdi7",
   "metadata": {
    "id": "E5vJOngiYdi7"
   },
   "source": [
    "# **Building a Conversational Chatbot with Langchain**\n",
    "\n",
    "# **Description:**\n",
    "In this activity, let's walk through the process of using LangChain, an open-source framework that enables the development of applications with large language models (LLMs) like OpenAI’s GPT-3.5-turbo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d12b7",
   "metadata": {},
   "source": [
    "Setup & Environment\n",
    "What this does:\n",
    "\n",
    "Installs modern LangChain packages and friends\n",
    "\n",
    "Pins sane minimum versions to avoid API drift\n",
    "\n",
    "Includes Gradio for the web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d263ad",
   "metadata": {},
   "source": [
    "Imports & Config\n",
    "What this does:\n",
    "\n",
    "Loads env vars from .env (or uses values you assign here)\n",
    "\n",
    "Centralizes all tunables (model, chunking, vectorstore path)\n",
    "\n",
    "Verifies your OpenAI project-scoped key is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6155011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U langchain>=0.2.10 langchain-community>=0.2.10 langchain-openai>=0.2.7 langchain-text-splitters>=0.2.2 chromadb>=0.5.5 pypdf>=4.2.0 python-dotenv>=1.0.1 gradio>=4.44.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1) Load environment variables from .env if present\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 2) Direct assignment for demo (OK for local/classroom use, NOT for public repos)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# If you want to be explicit about API base:\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n",
    "\n",
    "# Normalize env and make base explicit for the new OpenAI SDK\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY\"].strip()\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
    "\n",
    "\n",
    "# 3) Config — tweak as needed\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(os.getenv(\"PDF_DIR\", \"data\"))\n",
    "PERSIST_DIR = Path(os.getenv(\"CHROMA_DIR\", \"chroma_index\"))\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# RAG chunking\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", 1000))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", 150))\n",
    "TOP_K = int(os.getenv(\"TOP_K\", 4))\n",
    "\n",
    "# Basic validation\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.strip():\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set. Put it in a .env file or assign in the cell above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2a18b",
   "metadata": {},
   "source": [
    "LangChain Primitives\n",
    "What this does:\n",
    "\n",
    "Uses modern import locations (post-LangChain 0.2)\n",
    "\n",
    "Defines a clean, idempotent index builder\n",
    "\n",
    "Persists Chroma so you don’t rebuild every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d403237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: OpenAI failed → AuthenticationError(\"Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*********************************************************************************************.etc. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\")\n",
      "Falling back to SimulatedLLM (local).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "\n",
    "API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "BASE_URL = os.environ.get(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "import os, textwrap\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Pull env each time so the cell is self-contained\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
    "BASE_URL = os.environ.get(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "\n",
    "def format_docs(docs: List[Any]) -> str:\n",
    "    if not docs:\n",
    "        return \"NO CONTEXT AVAILABLE.\"\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[{i+1}] {d.metadata.get('source','unknown')} — p.{d.metadata.get('page','?')}\\n{d.page_content}\"\n",
    "        for i, d in enumerate(docs)\n",
    "    )\n",
    "\n",
    "# Embeddings with resilient fallback\n",
    "def make_embeddings(prefer_openai: bool = True):\n",
    "    if prefer_openai:\n",
    "        try:\n",
    "            eb = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=API_KEY, base_url=BASE_URL)\n",
    "            _ = eb.embed_documents([\"ping\"])\n",
    "            print(\"Embeddings: OpenAI OK\")\n",
    "            return eb\n",
    "        except Exception as e:\n",
    "            print(\"Embeddings: OpenAI failed →\", repr(e))\n",
    "            print(\"Falling back to FakeEmbeddings (local, deterministic).\")\n",
    "    from langchain_community.embeddings import FakeEmbeddings\n",
    "    return FakeEmbeddings(size=1536)\n",
    "\n",
    "# Build/load vectorstore\n",
    "def get_or_build_vectorstore(pdf_paths: List[Path], persist_dir: Path) -> Chroma:\n",
    "    try:\n",
    "        vs = Chroma(persist_directory=str(persist_dir), embedding_function=make_embeddings())\n",
    "        if vs._collection.count() > 0:\n",
    "            return vs\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf in pdf_paths:\n",
    "        loader = PyPDFLoader(str(pdf))\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source\"] = pdf.name\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True)\n",
    "    splits = splitter.split_documents(all_docs)\n",
    "\n",
    "    vs = Chroma.from_documents(documents=splits, embedding=make_embeddings(), persist_directory=str(persist_dir))\n",
    "    vs.persist()\n",
    "    return vs\n",
    "\n",
    "# LLM with robust fallback to a local simulator\n",
    "class SimulatedLLM:\n",
    "    model_name = \"simulated-llm\"\n",
    "    def invoke(self, prompt: str):\n",
    "        # Very small, deterministic response that uses the injected context\n",
    "        text = textwrap.dedent(\"\"\"\\\n",
    "            Using the provided context, here is a concise answer:\n",
    "\n",
    "            - I synthesized key points from the retrieved chunks.\n",
    "            - If details are missing, the source material did not cover them.\n",
    "            - See the cited sources below for provenance.\n",
    "\n",
    "            \"\"\")\n",
    "        class Obj: \n",
    "            def __init__(self, content): self.content = content\n",
    "        return Obj(text)\n",
    "\n",
    "def make_llm():\n",
    "    try:\n",
    "        llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.2, api_key=API_KEY, base_url=BASE_URL, timeout=60, max_retries=2)\n",
    "        _ = llm.invoke(\"ping\")\n",
    "        print(f\"LLM: {OPENAI_MODEL} OK\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(\"LLM: OpenAI failed →\", repr(e))\n",
    "        print(\"Falling back to SimulatedLLM (local).\")\n",
    "        return SimulatedLLM()\n",
    "\n",
    "llm = make_llm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c7e66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3A — Seed corpus utilities (RAG without PDFs)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "SEED_MINI_CORPUS = True  # set False when you switch to real PDFs\n",
    "\n",
    "# (source, page, text)\n",
    "SEED_TEXTS = [\n",
    "    (\"seed_rag.txt\", 1, \"Retrieval-Augmented Generation (RAG) improves factual accuracy by constraining the model to respond using retrieved context chunks.\"),\n",
    "    (\"seed_rag.txt\", 2, \"A typical RAG pipeline: ingest -> chunk -> embed -> index -> retrieve -> compose prompt with citations -> generate.\"),\n",
    "    (\"seed_langchain.txt\", 1, \"LangChain’s LCEL composes runnables like retrievers, prompts, and LLMs into a single graph you can invoke or stream.\"),\n",
    "    (\"seed_langchain.txt\", 2, \"Use ChatOpenAI for chat models and OpenAIEmbeddings for embedding; community vectorstores like Chroma handle local persistence.\"),\n",
    "    (\"seed_chroma.txt\", 1, \"Chroma is a lightweight vector store suitable for local demos. Persist the index to disk to avoid rebuilding every run.\"),\n",
    "    (\"seed_eval.txt\", 1, \"Quality levers in RAG include chunk_size, chunk_overlap, retriever k, and re-ranking. Start with k=4 and chunk_size≈1000.\"),\n",
    "]\n",
    "\n",
    "def build_seed_vectorstore(persist_dir: Path) -> Chroma:\n",
    "    docs = [Document(page_content=t, metadata={\"source\": src, \"page\": pg}) for (src, pg, t) in SEED_TEXTS]\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True)\n",
    "    splits = splitter.split_documents(docs)\n",
    "    vs = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=make_embeddings(),\n",
    "        persist_directory=str(persist_dir),\n",
    "    )\n",
    "    vs.persist()\n",
    "    return vs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6355ffc0",
   "metadata": {},
   "source": [
    "oint at Your PDFs\n",
    "What this does:\n",
    "\n",
    "Lists PDFs under data/ (or your custom PDF_DIR)\n",
    "\n",
    "Builds or loads the Chroma index\n",
    "\n",
    "Creates a retriever\n",
    "\n",
    "How to use:\n",
    "\n",
    "Drop one or more PDFs into ./data/ before running.\n",
    "\n",
    "Re-run this cell whenever you change the corpus. Delete ./chroma_index/ to fully rebuild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54e684e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed] No PDFs found. Building tiny seed corpus to demonstrate RAG…\n",
      "Embeddings: OpenAI failed → AuthenticationError(\"Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*********************************************************************************************.etc. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\")\n",
      "Falling back to FakeEmbeddings (local, deterministic).\n",
      "Vectorstore: ready @ C:\\Users\\jim\\OneDrive\\Desktop\\AMJ Group\\Teaching\\Class Materials\\AGS_Advanced_Generative_AI_Building_LLM_Applications_ILT_Material\\Guided_Practice\\Lesson_04_LangChain_for_LLM_Application_Development_Part_2\\chroma_index\n",
      "Mode: Seed corpus (6 snippets)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "\n",
    "# Discover PDFs (recursive, case-insensitive)\n",
    "DATA_DIR = DATA_DIR.expanduser().resolve()\n",
    "patterns = (\"*.pdf\", \"*.PDF\")\n",
    "pdfs = []\n",
    "for pat in patterns:\n",
    "    pdfs.extend(DATA_DIR.rglob(pat))\n",
    "\n",
    "try:\n",
    "    if pdfs:\n",
    "        print(\"Indexing PDFs:\")\n",
    "        for p in sorted(pdfs):\n",
    "            print(\" -\", p)\n",
    "        vectorstore = get_or_build_vectorstore(pdfs, PERSIST_DIR)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        mode = f\"PDFs ({len(pdfs)} files)\"\n",
    "    elif SEED_MINI_CORPUS:\n",
    "        print(\"[Seed] No PDFs found. Building tiny seed corpus to demonstrate RAG…\")\n",
    "        vectorstore = build_seed_vectorstore(PERSIST_DIR)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        mode = f\"Seed corpus ({len(SEED_TEXTS)} snippets)\"\n",
    "    else:\n",
    "        print(\"[Bypass] No PDFs and seeding disabled. Running with NO CONTEXT.\")\n",
    "        vectorstore = None\n",
    "        retriever = lambda q: []\n",
    "        mode = \"No context\"\n",
    "\n",
    "    print(\"Vectorstore:\", \"ready @ \" + str(PERSIST_DIR.resolve()) if vectorstore else \"disabled\")\n",
    "    print(\"Mode:\", mode)\n",
    "\n",
    "except Exception as e:\n",
    "    # Final safety net: in-memory FAISS so the demo always runs\n",
    "    print(\"Index build failed:\", repr(e))\n",
    "    print(\"Falling back to in-memory FAISS.\")\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    from langchain_core.documents import Document\n",
    "\n",
    "    docs = [Document(page_content=t, metadata={\"source\": src, \"page\": pg}) for (src, pg, t) in SEED_TEXTS] if SEED_MINI_CORPUS else []\n",
    "    splits = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True).split_documents(docs) if docs else []\n",
    "    embeddings = make_embeddings(prefer_openai=False)\n",
    "    if splits:\n",
    "        vectorstore = FAISS.from_documents(splits, embeddings)  # in-memory\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        mode = \"Seed corpus (FAISS fallback)\"\n",
    "    else:\n",
    "        vectorstore = None\n",
    "        retriever = lambda q: []\n",
    "        mode = \"No context (FAISS fallback)\"\n",
    "    print(\"Vectorstore:\", \"in-memory FAISS\" if vectorstore else \"disabled\")\n",
    "    print(\"Mode:\", mode)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0a073",
   "metadata": {},
   "source": [
    "RAG Prompt, Chain, and answer()\n",
    "What this does:\n",
    "\n",
    "Builds a grounded prompt that cites sources.\n",
    "\n",
    "Wires retriever → prompt → llm → text.\n",
    "\n",
    "answer(question) returns answer text, sources, and latency, and tags the current mode (Seed, PDFs, or No context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4acbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a precise technical assistant. Answer the user’s question using ONLY the provided context.\n",
    "If the answer cannot be found in the context, say you don’t know.\n",
    "Cite sources as [#] matching the provided context items.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", SYSTEM_PROMPT), (\"human\", \"{question}\")])\n",
    "\n",
    "# If llm is a real ChatOpenAI runnable, use it; otherwise use a local string-producing runnable.\n",
    "if isinstance(llm, ChatOpenAI):\n",
    "    model_runnable = llm\n",
    "else:\n",
    "    model_runnable = RunnableLambda(\n",
    "        lambda _msgs: (\n",
    "            \"Using the provided context, here is a concise answer.\\n\\n\"\n",
    "            \"- Key points synthesized from retrieved chunks.\\n\"\n",
    "            \"- If details are missing, the context did not cover them.\\n\"\n",
    "            \"- See Sources for provenance.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chain: retriever → formatter → prompt → model (real or simulated) → string\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_runnable\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def answer(question: str) -> Dict[str, Any]:\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    # Retrieval (graceful if bypass lambda)\n",
    "    try:\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "    except AttributeError:\n",
    "        docs = []\n",
    "\n",
    "    sources = [\n",
    "        {\n",
    "            \"source\": d.metadata.get(\"source\", \"unknown\"),\n",
    "            \"page\": d.metadata.get(\"page\"),\n",
    "            \"snippet\": (d.page_content[:280] + \"…\") if len(d.page_content) > 280 else d.page_content,\n",
    "        }\n",
    "        for d in docs\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        text = rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        text = f\"Request failed while calling the chat model.\\nError: {repr(e)}\"\n",
    "        return {\"answer\": text, \"sources\": sources, \"latency_s\": round(time.time() - t0, 3)}\n",
    "\n",
    "    mode_tag = \"Seed corpus\" if docs and any(s[\"source\"].startswith(\"seed_\") for s in sources) else (\"PDFs\" if docs else \"No context\")\n",
    "    text += f\"\\n\\n---\\nMode: {mode_tag}.\"\n",
    "    return {\"answer\": text, \"sources\": sources, \"latency_s\": round(time.time() - t0, 3)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68848afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the provided context, here is a concise answer.\n",
      "\n",
      "- Key points synthesized from retrieved chunks.\n",
      "- If details are missing, the context did not cover them.\n",
      "- See Sources for provenance.\n",
      "\n",
      "---\n",
      "Mode: Seed corpus.\n"
     ]
    }
   ],
   "source": [
    "print(answer(\"Give a 2-bullet summary of the RAG pipeline. Cite sources.\")[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ea7cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the provided context, here is a concise answer.\n",
      "\n",
      "- Key points synthesized from retrieved chunks.\n",
      "- If details are missing, the context did not cover them.\n",
      "- See Sources for provenance.\n",
      "\n",
      "---\n",
      "Mode: Seed corpus.\n",
      "\n",
      "Sources:\n",
      "- seed_langchain.txt p.1: LangChain’s LCEL composes runnables like retrievers, prompts, and LLMs into a single graph you can invoke or stream.\n",
      "- seed_rag.txt p.2: A typical RAG pipeline: ingest -> chunk -> embed -> index -> retrieve -> compose prompt with citations -> generate.\n",
      "- seed_langchain.txt p.1: LangChain’s LCEL composes runnables like retrievers, prompts, and LLMs into a single graph you can invoke or stream.\n",
      "- seed_rag.txt p.1: Retrieval-Augmented Generation (RAG) improves factual accuracy by constraining the model to respond using retrieved cont\n",
      "\n",
      "Latency: 0.005s\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "\n",
    "res = answer(\"Explain the typical RAG embedding mechanism. Cite sources.\")\n",
    "print(res[\"answer\"])\n",
    "print(\"\\nSources:\")\n",
    "for s in res[\"sources\"]:\n",
    "    snippet_clean = s['snippet'][:120].replace(\"\\n\", \" \")\n",
    "    print(f\"- {s['source']} p.{s['page']}: {snippet_clean}\")\n",
    "print(f\"\\nLatency: {res['latency_s']}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53cf6105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jim\\AppData\\Local\\Temp\\ipykernel_8172\\3490168718.py:25: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=450)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7\n",
    "\n",
    "# Cell 7 — Reliable Gradio UI (no ChatInterface)\n",
    "\n",
    "import gradio as gr\n",
    "import traceback, sys\n",
    "\n",
    "def respond(message, history):\n",
    "    try:\n",
    "        res = answer(message)\n",
    "        cites = \"\\n\".join(f\"- {s['source']} p.{s['page']}\" for s in res.get(\"sources\", [])) or \"(none)\"\n",
    "        reply = res.get(\"answer\", \"No answer produced.\") + \"\\n\\n---\\nSources:\\n\" + cites\n",
    "        history = history + [(message, reply)]\n",
    "        return \"\", history\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        print(\"UI handler error:\\n\", tb, file=sys.stderr)\n",
    "        reply = f\"Handler error: {e!r}\\n\\n— check notebook output for traceback.\"\n",
    "        history = history + [(message, reply)]\n",
    "        return \"\", history\n",
    "\n",
    "with gr.Blocks(title=\"LangChain RAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# LangChain RAG Chatbot\\nAsk questions grounded in your corpus (seeded for now).\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=450)\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(placeholder=\"Ask something…\", scale=5)\n",
    "        send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "\n",
    "    status = gr.Markdown(\n",
    "        f\"**PDF directory:** `{DATA_DIR}`  \\n\"\n",
    "        f\"**Chroma directory:** `{PERSIST_DIR}`  \\n\"\n",
    "        f\"**Model:** `{OPENAI_MODEL}`  \\n\"\n",
    "        f\"**Embedding model:** `{EMBEDDING_MODEL}`  \\n\"\n",
    "        f\"**k:** {TOP_K}, **chunk_size:** {CHUNK_SIZE}, **overlap:** {CHUNK_OVERLAP}\"\n",
    "    )\n",
    "\n",
    "    send.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])  # press Enter to send\n",
    "\n",
    "demo.launch(share=False, show_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "534b497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_URL: https://api.openai.com/v1\n",
      "OpenAI Embeddings probe failed: AuthenticationError(\"Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*********************************************************************************************.etc. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\")\n",
      "Chat probe failed: AuthenticationError(\"Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*********************************************************************************************.etc. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\")\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — API client sanity probe\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "print(\"BASE_URL:\", os.environ.get(\"OPENAI_BASE_URL\", \"unset\"))\n",
    "\n",
    "# Embeddings probe (may fail if FakeEmbeddings is currently used — that’s fine)\n",
    "try:\n",
    "    length = len(OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=API_KEY, base_url=BASE_URL).embed_documents([\"ok\"])[0])\n",
    "    print(\"OpenAI Embedding vector length:\", length)\n",
    "except Exception as e:\n",
    "    print(\"OpenAI Embeddings probe failed:\", repr(e))\n",
    "\n",
    "# Chat probe\n",
    "try:\n",
    "    msg = ChatOpenAI(model=OPENAI_MODEL, temperature=0, api_key=API_KEY, base_url=BASE_URL).invoke(\"ping\").content\n",
    "    print(\"Chat model response:\", msg)\n",
    "except Exception as e:\n",
    "    print(\"Chat probe failed:\", repr(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03add8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Mq9A_sITsLiw",
   "metadata": {
    "id": "Mq9A_sITsLiw"
   },
   "source": [
    "# **Steps to Perform:**\n",
    "\n",
    "1. Set up the Environment\n",
    "2. Define a Document Loader\n",
    "3. Create a Document Splitter\n",
    "4. Embed the Text and Save it in Vector Stores\n",
    "5. Create a Retrieval Function\n",
    "6. Run the Chatbot and Understand the Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yz1jxPDoYeyz",
   "metadata": {
    "id": "Yz1jxPDoYeyz"
   },
   "source": [
    "# **Step 1: Set up the Environment**\n",
    "\n",
    "\n",
    "*   Import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357726b",
   "metadata": {
    "id": "2357726b"
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import os\n",
    "import openai\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a1db2",
   "metadata": {
    "id": "561a1db2"
   },
   "source": [
    "# **Step 2: Define a Document Loader**\n",
    "\n",
    "\n",
    "\n",
    "*  Use a document loader like PyPDF to load information from a PDF file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b5328",
   "metadata": {
    "id": "431b5328"
   },
   "outputs": [],
   "source": [
    "#Using PyPDF\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "Doc_loader = PyPDFLoader(\"bcg-2022-annual-sustainability-report-apr-2023.pdf\")\n",
    "extracted_text = Doc_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e6367",
   "metadata": {
    "id": "381e6367"
   },
   "source": [
    "# **Step 3: Create a Document Splitter**\n",
    "\n",
    "\n",
    "*   Break down big pieces of text into smaller parts using text splitters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c60d0",
   "metadata": {
    "id": "997c60d0"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter  = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "splitted_text=text_splitter.split_documents(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074db8d4",
   "metadata": {
    "id": "074db8d4"
   },
   "source": [
    "# **Step 4: Embed the Text and Save it in Vector Stores**\n",
    "\n",
    "\n",
    "*  Arrange a place to store and organize the text splits to make\n",
    " them searchable.\n",
    "*  Employ OpenAIEmbeddings to create a pretrained model instance, saving the results in a specified directory path.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bae9d",
   "metadata": {
    "id": "0a0bae9d"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3ce63",
   "metadata": {
    "id": "b5a3ce63"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f9f4e",
   "metadata": {
    "id": "be3f9f4e"
   },
   "outputs": [],
   "source": [
    "persist_directory = \"chroma_vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b380f97",
   "metadata": {
    "id": "5b380f97"
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splitted_text,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d601da",
   "metadata": {
    "id": "00d601da"
   },
   "source": [
    "# **Step 5: Create a Retrieval Function**\n",
    "\n",
    "\n",
    "*   Retrieve pertinent data from storage based on user input using a retriever.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222db36",
   "metadata": {
    "id": "9222db36"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95613f21",
   "metadata": {
    "id": "95613f21"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "Retriever_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3PgxILTAZOZA",
   "metadata": {
    "id": "3PgxILTAZOZA"
   },
   "source": [
    "# **Step 6: Run the Chatbot and Understand the Code**\n",
    "\n",
    "\n",
    "*   Set up the chatbot, run it and\n",
    "interact with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92170267",
   "metadata": {
    "id": "92170267",
    "outputId": "b4011b44-79ba-4467-eb05-9b230fe4b2ae"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "        query = input(\"\\nEnter a query: \")\n",
    "        if query == \"exit\":\n",
    "            break\n",
    "        if query.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        # Get the answer from the chain\n",
    "        start = time.time()\n",
    "\n",
    "        res=Retriever_chain(query)\n",
    "\n",
    "\n",
    "        # Print the result\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"\\n\\n> Question:\")\n",
    "\n",
    "        print(query)\n",
    "\n",
    "        print(f\"\\n> Answer (took {round(end - start, 2)} s.):\")\n",
    "\n",
    "        print(res['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dGmMKkoZUPy",
   "metadata": {
    "id": "6dGmMKkoZUPy"
   },
   "source": [
    "# **Conclusion:**\n",
    "The code will ask the user to enter a query, get an answer from the chatbot, and print it along with how long it took to get the answer. The user can exit the chatbot by typing **exit**. If the user enters an empty query, the chatbot will ask for another query. This is the final step in creating and running the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689b9d5-5942-4c78-9712-1e24cce3c716",
   "metadata": {
    "id": "2689b9d5-5942-4c78-9712-1e24cce3c716"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
